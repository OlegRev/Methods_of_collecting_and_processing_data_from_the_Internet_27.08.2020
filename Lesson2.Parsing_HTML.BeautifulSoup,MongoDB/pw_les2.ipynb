{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Описание задания\n",
    "    Необходимо собрать информацию о вакансиях на вводимую должность\n",
    "    (используем input или через аргументы) с сайтов Superjob и HH.\n",
    "    Приложение должно анализировать несколько страниц сайта\n",
    "    (также вводим через input или аргументы).\n",
    "    Получившийся список должен содержать в себе минимум:\n",
    "        Наименование вакансии.\n",
    "        Предлагаемую зарплату (отдельно минимальную и максимальную).\n",
    "        Ссылку на саму вакансию.\n",
    "        Сайт, откуда собрана вакансия.\n",
    "    По желанию можно добавить ещё параметры вакансии\n",
    "    (например, работодателя и расположение)\n",
    "    Структура должна быть одинаковая для вакансий с обоих сайтов.\n",
    "    Общий результат можно вывести с помощью dataFrame через pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restruct_compensation(compensation: str):\n",
    "    \"\"\"Функция обработки укзателя зарплаты для\n",
    "    разделения на минимальную, максимальную \n",
    "    и денежную единицу измерения \n",
    "    с использованием регулярных выражений\n",
    "    (функция содержит import re)\n",
    "\n",
    "    Args:\n",
    "        compensation (str):\n",
    "        тестировалось на выражениях:\n",
    "            ['30000-70000 руб.', 'до 96000 руб.', 'от 150000 руб.',\n",
    "             'от 150000 руб. до 200000 руб.', '200000-450000 KZT',\n",
    "             '2500-4000 USD', '3000000'\n",
    "            ]\n",
    "\n",
    "    Returns:\n",
    "        result (dict):\n",
    "            result.keys(): ['min_compensation',\n",
    "                            'max_compensation',\n",
    "                            'currency_compensation'\n",
    "                           ]\n",
    "    \"\"\"\n",
    "    import re\n",
    "    if compensation.isdigit():\n",
    "        result = {'min_compensation' : int(compensation),\n",
    "                  'max_compensation': int(compensation),\n",
    "                  'currency_compensation' : None\n",
    "                 }\n",
    "        return result\n",
    "\n",
    "    elif re.search(r'^до',compensation):\n",
    "        re_compensation = re.search(r'().*?(\\d{1,}).*?(\\w{3}).*?', compensation)\n",
    "    elif '-'in compensation or ('от' in compensation and 'до' in compensation):\n",
    "        re_compensation = re.search(r'.*?(\\d{1,}).*?(\\d{1,}).*?(\\w{3}).*?', compensation)\n",
    "    elif 'от' in compensation and 'до' not in compensation:\n",
    "        re_compensation = re.search(r'.*?(\\d{1,}).*?()(\\w{3}).*?', compensation)\n",
    "    \n",
    "    result = {'min_compensation' : re_compensation.group(1),\n",
    "              'max_compensation': re_compensation.group(2),\n",
    "              'currency_compensation' : re_compensation.group(3)\n",
    "             }\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def parser_vacancy_item(required_vacancy:'bs4.element.Tag', website: str, parser_params: dict):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        required_vacancy (bs4.element.Tag): [description]\n",
    "        website (str): [https://spb.hh.ru or https://www.superjob.ru]\n",
    "        parser_params (dict): \n",
    "        [Описание для hh.ru]: {'vacancy_header_blok' : ['div', 'class', 'vacancy-serp-item__row_header'],\n",
    "                               'vacancy_info' : ['div', 'class', 'vacancy-serp-item__info'],\n",
    "                               'vacancy_sidebar_compensation' : ['div', 'class', 'vacancy-serp-item__sidebar'],\n",
    "                               'vacancy_link' : ['a', 'class', 'bloko-link'],\n",
    "                               'company_metainfo' : ['div', 'class', 'vacancy-serp-item__meta-info'],\n",
    "                               'company_link' : ['a', 'data-qa', 'vacancy-serp__vacancy-employer'],\n",
    "                               'company_name' : ['a', 'data-qa', 'vacancy-serp__vacancy-employer'],\n",
    "                               'company_location' : ['span', 'data-qa', 'vacancy-serp__vacancy-address']\n",
    "                              }  \n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    vacancy_data={}\n",
    "    vacancy_data['website'] = website\n",
    "    vacancy_header_blok = required_vacancy.find(parser_params['vacancy_header_blok'][0],\n",
    "                                                {parser_params['vacancy_header_blok'][1]:\n",
    "                                                 parser_params['vacancy_header_blok'][2]})\n",
    "    vacancy_info = vacancy_header_blok.find(parser_params['vacancy_info'][0],\n",
    "                                            {parser_params['vacancy_info'][1]:\n",
    "                                             parser_params['vacancy_info'][2]})\n",
    "    vacancy_sidebar_compensation = vacancy_header_blok.find(parser_params['vacancy_sidebar_compensation'][0],\n",
    "                                                            {parser_params['vacancy_sidebar_compensation'][1]:\n",
    "                                                             parser_params['vacancy_sidebar_compensation'][2]})\n",
    "    vacancy_name = vacancy_info.getText()\n",
    "    vacancy_link = vacancy_info.find(parser_params['vacancy_link'][0],\n",
    "                                     {parser_params['vacancy_link'][1]\n",
    "                                     : parser_params['vacancy_link'][2]}).get('href')\n",
    "    \n",
    "    vacancy_data['name'] = vacancy_name\n",
    "    \n",
    "    if website == 'https://hh.ru':\n",
    "        vacancy_data['link'] = vacancy_link\n",
    "        company_metainfo = required_vacancy.find_all(parser_params['company_metainfo'][0],\n",
    "                                                     {parser_params['company_metainfo'][1]\n",
    "                                                     :parser_params['company_metainfo'][2]})\n",
    "        try:\n",
    "            company_link = company_metainfo[0].find(parser_params['company_link'][0],\n",
    "                                                    {parser_params['company_link'][1]\n",
    "                                                    : parser_params['company_link'][2]}).get('href')\n",
    "            vacancy_data['company_link'] = company_link  \n",
    "        except:\n",
    "            vacancy_data['company_link'] = None\n",
    "            \n",
    "    elif website == 'https://www.superjob.ru':\n",
    "        vacancy_data['link'] = website + vacancy_link\n",
    "        company_metainfo = required_vacancy.find(parser_params['company_metainfo'][0],\n",
    "                                                 {parser_params['company_metainfo'][1]\n",
    "                                                 :parser_params['company_metainfo'][2]})\n",
    "        company_metainfo = company_metainfo.find_all(parser_params['company_metainfo'][3],\n",
    "                                                     {parser_params['company_metainfo'][4]\n",
    "                                                     :parser_params['company_metainfo'][5]})\n",
    "        try:\n",
    "            company_link = company_metainfo[0].find(parser_params['company_link'][0],\n",
    "                                                    {parser_params['company_link'][1]\n",
    "                                                    : parser_params['company_link'][2]}).get('href')\n",
    "            vacancy_data['company_link'] = website + company_link  \n",
    "        except:\n",
    "            vacancy_data['company_link'] = None\n",
    "            \n",
    "    try: \n",
    "        company_name = company_metainfo[0].find(parser_params['company_name'][0],\n",
    "                                                {parser_params['company_name'][1]\n",
    "                                                : parser_params['company_name'][2]}).getText()\n",
    "        vacancy_data['company_name'] = company_name\n",
    "    except:\n",
    "        vacancy_data['company_name'] = None\n",
    "    try:\n",
    "        company_location = company_metainfo[1].find(parser_params['company_location'][0],\n",
    "                                                    {parser_params['company_location'][1]\n",
    "                                                    : parser_params['company_location'][2]}).getText()\n",
    "        vacancy_data['company_location'] = company_location\n",
    "    except:\n",
    "        vacancy_data['company_location'] = None\n",
    "    try:\n",
    "        vacancy_compensation = vacancy_sidebar_compensation.getText()\n",
    "        vacancy_compensation = vacancy_compensation.replace('\\xa0', '')\n",
    "        vacancy_compensation = restruct_compensation(vacancy_compensation)\n",
    "        vacancy_data['min_compensation'] = vacancy_compensation['min_compensation']\n",
    "        vacancy_data['max_compensation'] = vacancy_compensation['max_compensation']\n",
    "        vacancy_data['currency_compensation'] = vacancy_compensation['currency_compensation']\n",
    "    except:\n",
    "        vacancy_data['min_compensation'] = None\n",
    "        vacancy_data['max_compensation'] = None\n",
    "        vacancy_data['currency_compensation'] = None\n",
    "            \n",
    "    return vacancy_data\n",
    "\n",
    "\n",
    "def hh_get_vacancies(required_vacancy: str, headers: dict, num_area: int = 0):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        required_vacancy (str): [description]\n",
    "        num_area (int, optional): [description]. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    vacancies_data = []\n",
    "    main_link = 'https://hh.ru'\n",
    "    second_link ='search/vacancy?'\n",
    "    params_hh = {'fromSearchLine': 'true',\n",
    "                'L_is_autosearch':'false',\n",
    "                 'area': num_area,\n",
    "                 'enable_snippets':'true',\n",
    "                 'salary': '',\n",
    "                 'st':'searchVacancy',\n",
    "                 'text': required_vacancy,\n",
    "                 'page': ''\n",
    "                 }\n",
    "    # hh\n",
    "    parser_params = {'vacancy_header_blok' : ['div', 'class', 'vacancy-serp-item__row_header'],\n",
    "                    'vacancy_info' : ['div', 'class', 'vacancy-serp-item__info'],\n",
    "                    'vacancy_sidebar_compensation' : ['div', 'class', 'vacancy-serp-item__sidebar'],\n",
    "                    'vacancy_link' : ['a', 'class', 'bloko-link'],\n",
    "                    'company_metainfo' : ['div', 'class', 'vacancy-serp-item__meta-info'],\n",
    "                    'company_link' : ['a', 'data-qa', 'vacancy-serp__vacancy-employer'],\n",
    "                    'company_name' : ['a', 'data-qa', 'vacancy-serp__vacancy-employer'],\n",
    "                    'company_location' : ['span', 'data-qa', 'vacancy-serp__vacancy-address']\n",
    "                    }\n",
    "\n",
    "    full_link = main_link + second_link\n",
    "    response_page = requests.get(full_link, params=params_hh, headers=headers)\n",
    "    \n",
    "    if response_page.ok:\n",
    "        soup = bs(response_page.text,'html.parser')\n",
    "        try:\n",
    "            pages_blok = soup.find('div', {'data-qa': 'pager-block'})\n",
    "            pages_list = pages_blok.find_all('a', {'data-qa': 'pager-page'})\n",
    "            last_page_number = int(pages_list[-1].getText())\n",
    "        except:\n",
    "            last_page_number = 1\n",
    "        \n",
    "\n",
    "    for page in range(last_page_number):\n",
    "        params_hh['page'] = page\n",
    "        response_page = requests.get(full_link, params=params_hh, headers=headers)\n",
    "        if response_page.ok:\n",
    "            soup = bs(response_page.text,'html.parser')\n",
    "    \n",
    "            vacancies_serp = soup.find('div', {'data-qa': 'vacancy-serp__results'}).find_all('div', {'class': 'vacancy-serp-item'})\n",
    "            for vacancy in vacancies_serp:\n",
    "                vacancies_data.append(parser_vacancy_item(vacancy,parser_params=parser_params, website=main_link))\n",
    "    \n",
    "    return vacancies_data\n",
    "\n",
    "\n",
    "def sj_get_vacancies(required_vacancy: str, headers: dict, num_area: int = 0):\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Args:\n",
    "        required_vacancy (str): [description]\n",
    "        num_area (int, optional): [description]. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    vacancies_data = []\n",
    "    # superjob\n",
    "    main_link = 'https://www.superjob.ru'\n",
    "    second_link ='/vacancy/search/'\n",
    "    params_hh = {'keywords': required_vacancy,\n",
    "                 'noGeo': '1',\n",
    "                 'page': ''\n",
    "                }\n",
    "    parser_params = {'vacancy_header_blok' : ['div', 'class', 'jNMYr GPKTZ _1tH7S'],\n",
    "                    'vacancy_info' : ['div', 'class', '_3mfro PlM3e _2JVkc _3LJqf'],\n",
    "                    'vacancy_sidebar_compensation' : ['span', 'class', '_1OuF_ _1qw9T f-test-text-company-item-salary'],\n",
    "                    'vacancy_link' : ['a', 'target', '_blank'],\n",
    "                    'company_metainfo' : ['div', 'class', '_3_eyK _3P0J7 _9_FPy', 'div', 'class', '_2g1F-'],\n",
    "                    'company_link' : ['a', 'target', '_self'],\n",
    "                    'company_name' : ['a', 'target', '_self'],\n",
    "                    'company_location' : ['span', 'class', '_3mfro f-test-text-company-item-location _9fXTd _2JVkc _2VHxz']\n",
    "                    }\n",
    "\n",
    "    full_link = main_link + second_link\n",
    "    response_page = requests.get(full_link, params=params_hh, headers=headers)\n",
    "   \n",
    "    if response_page.ok:\n",
    "        soup = bs(response_page.text,'html.parser')\n",
    "        try:\n",
    "            soup = bs(response_page.text,'html.parser')\n",
    "            pages_blok = soup.find('div', {'class': '_3zucV L1p51 undefined _1Fty7 _2tD21 _3SGgo'})\n",
    "            pages_list = pages_blok.find_all('a', {'target': '_self'})\n",
    "            last_page_number = int(pages_list[-2].getText())\n",
    "        except:\n",
    "            last_page_number = 1\n",
    "        \n",
    "    for page in range(1, last_page_number+1):\n",
    "        params_hh['page'] = page\n",
    "        response_page = requests.get(full_link, params=params_hh, headers=headers)\n",
    "        if response_page.ok:\n",
    "            soup = bs(response_page.text,'html.parser')\n",
    "    \n",
    "            vacancies_serp = soup.find('div', {'class': '_1ID8B'}).find_all('div', {'class': 'Fo44F QiY08 LvoDO'})\n",
    "            for vacancy in vacancies_serp:\n",
    "                vacancies_data.append(parser_vacancy_item(vacancy,parser_params=parser_params, website=main_link))\n",
    "    \n",
    "    return vacancies_data\n",
    "\n",
    "\n",
    "def get_df_vacancies(required_vacancy: str, headers: dict, vacancies_data = []):\n",
    "    \"\"\"Функция преобразования в датафрейм полученых вакансий  с сайтов:\n",
    "        'hh.ru', 'superjob.ru'\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        required_vacancy (str): [description]\n",
    "        vacancies (list, optional): [description]. Defaults to [].\n",
    "\n",
    "    Returns:\n",
    "        df[<class 'pandas.core.frame.DataFrame'>]: [description]\n",
    "    \"\"\"\n",
    "    vacancies_data.extend(hh_get_vacancies(required_vacancy, headers))\n",
    "    vacancies_data.extend(sj_get_vacancies(required_vacancy, headers))\n",
    "    df = pd.DataFrame(vacancies_data)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_vacancy = 'Data scientist'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) '\n",
    "           'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.135 Safari/537.36'\n",
    "          }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='spb.hh.rusearch', port=443): Max retries exceeded with url: /vacancy?L_is_autosearch=false&area=0&enable_snippets=true&salary=&st=searchVacancy&text=Data+scientist&page= (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7ff32eee3b90>: Failed to establish a new connection: [Errno -2] Name or service not known'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -2] Name or service not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    993\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;31m# Add certificate verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m             raise NewConnectionError(\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Failed to establish a new connection: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             )\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.VerifiedHTTPSConnection object at 0x7ff32eee3b90>: Failed to establish a new connection: [Errno -2] Name or service not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    719\u001b[0m             retries = retries.increment(\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='spb.hh.rusearch', port=443): Max retries exceeded with url: /vacancy?L_is_autosearch=false&area=0&enable_snippets=true&salary=&st=searchVacancy&text=Data+scientist&page= (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7ff32eee3b90>: Failed to establish a new connection: [Errno -2] Name or service not known'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-22e41e0c6c80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_df_vacancies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequired_vacancy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequired_vacancy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-8f361293a8ff>\u001b[0m in \u001b[0;36mget_df_vacancies\u001b[0;34m(required_vacancy, headers, vacancies_data)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'pandas.core.frame.DataFrame'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \"\"\"\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0mvacancies_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhh_get_vacancies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequired_vacancy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m     \u001b[0mvacancies_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msj_get_vacancies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequired_vacancy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvacancies_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-8f361293a8ff>\u001b[0m in \u001b[0;36mhh_get_vacancies\u001b[0;34m(required_vacancy, headers, num_area)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0mfull_link\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_link\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msecond_link\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0mresponse_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_link\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams_hh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse_page\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='spb.hh.rusearch', port=443): Max retries exceeded with url: /vacancy?L_is_autosearch=false&area=0&enable_snippets=true&salary=&st=searchVacancy&text=Data+scientist&page= (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7ff32eee3b90>: Failed to establish a new connection: [Errno -2] Name or service not known'))"
     ]
    }
   ],
   "source": [
    "df = get_df_vacancies(required_vacancy=required_vacancy, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-00cf07b74dcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
